{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80df3838",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Append parent directory to Python path\n",
    "\n",
    "import pandas as pd\n",
    "from scripts.data_loader import load_data\n",
    "from scripts.plot_util import PlotHelper\n",
    "# from scripts.preprocess import preprocess_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abca0c0",
   "metadata": {},
   "source": [
    "load data basic eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"../data/raw_analyst_ratings.csv\")\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da917953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['headline']))\n",
    "print(df['headline'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Handling Null Values ---\n",
    "\n",
    "print(\"\\n--- 1. Null Value Check (Before Cleaning) ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# We focus on the 'headline' column since it's used for sentiment analysis.\n",
    "# Strategy: Drop rows where the 'headline' is null, as we can't analyze empty text.\n",
    "df.dropna(subset=['headline'], inplace=True)\n",
    "print(\"\\n--- Null Value Check (After Cleaning) ---\")\n",
    "print(f\"Remaining rows after dropping null headlines: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Date Structure and Time Series Preparation  ---\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    \n",
    "    # 1. Force conversion using 'ISO8601' format and ensure UTC conversion.\n",
    "    # Setting utc=True handles the mixed timezone offsets by normalizing all values to UTC \n",
    "    # and ensures the resulting column is a proper datetime dtype.\n",
    "    df['date'] = pd.to_datetime(df['date'], format='ISO8601', utc=True)\n",
    "    \n",
    "    print(\"\\n--- 2. Date Column Structure (After Conversion) ---\")\n",
    "    print(df['date'].head())\n",
    "    print(f\"Data type is now: {df['date'].dtype}\")\n",
    "    # 2. Set the Index\n",
    "    # Note: Since utc=True was used, the dates are already in UTC.\n",
    "    df.set_index('date', inplace=True)\n",
    "    print(\"\\nDataFrame Index is now set to Date (UTC).\")\n",
    "    print(df.head()) \n",
    "    \n",
    "else:\n",
    "    print(\"\\nWarning: Date column not found. Skipping date conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Duplicate and Consistency Checks ---\n",
    "\n",
    "# Check for duplicate rows across all columns\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nTotal duplicate rows found: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    # Drop duplicates, keeping the first instance\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Rows remaining after dropping duplicates: {df.shape[0]}\")\n",
    "\n",
    "# Ensure the 'headline' column is string type before analysis\n",
    "df['headline'] = df['headline'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d8f92",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate headline lengths\n",
    "df['headline_length'] = df['headline'].str.len()\n",
    "\n",
    "# Basic statistics\n",
    "headline_stats = df['headline_length'].describe()\n",
    "print(\"Headline Length Statistics:\")\n",
    "print(headline_stats)\n",
    "\n",
    "# Visualize with PlotHelper\n",
    "helper = PlotHelper()\n",
    "helper.histogram(df, column='headline_length', bins=20, title='Distribution of Headline Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b628e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count articles per publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "print(\"Number of Articles per Publisher:\")\n",
    "print(publisher_counts)\n",
    "\n",
    "# Visualize top 10 with PlotHelper\n",
    "top_publishers = publisher_counts.head(10).reset_index()\n",
    "top_publishers.columns = ['publisher', 'count']\n",
    "helper.bar(top_publishers, x='publisher', height='count', title='Top 10 Most Active Publishers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8efcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' is datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Group by date and count articles\n",
    "daily_counts = df.groupby(df['date'].dt.date).size().reset_index(name='count')\n",
    "daily_counts['date'] = pd.to_datetime(daily_counts['date'])\n",
    "print(\"Daily Article Counts:\")\n",
    "print(daily_counts.head())\n",
    "\n",
    "# Plot trends over time with PlotHelper\n",
    "helper.line_plot(daily_counts, x='date', y='count', title='Article Frequency Over Time')\n",
    "\n",
    "# Analyze by day of week\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "weekly_counts = df['day_of_week'].value_counts().reset_index()\n",
    "weekly_counts.columns = ['day', 'count']\n",
    "print(\"Articles by Day of Week:\")\n",
    "print(weekly_counts)\n",
    "\n",
    "# Visualize days with PlotHelper\n",
    "helper.bar(weekly_counts, x='day', y='count', title='Articles by Day of the Week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc275860",
   "metadata": {},
   "source": [
    "Text Analysis(Topic Modeling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Download NLTK resources if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and lowercase\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to headlines\n",
    "df['processed_headline'] = df['headline'].apply(preprocess_text)\n",
    "\n",
    "# 1. Extract common keywords using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))  # Unigrams and bigrams\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_headline'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "keywords = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "print(\"Top 20 Keywords/Phrases by TF-IDF:\")\n",
    "for word, score in keywords:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# 2. Topic Modeling with LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display topics\n",
    "print(\"\\nTop Topics (LDA):\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# 3. Identify significant events/phrases (e.g., \"FDA approval\", \"price target\")\n",
    "significant_phrases = ['fda approval', 'price target', 'earnings report', 'merger', 'acquisition', 'stock split', 'dividend']\n",
    "phrase_counts = Counter()\n",
    "for headline in df['processed_headline']:\n",
    "    for phrase in significant_phrases:\n",
    "        if phrase in headline:\n",
    "            phrase_counts[phrase] += 1\n",
    "\n",
    "print(\"\\nCounts of Significant Phrases:\")\n",
    "for phrase, count in phrase_counts.most_common():\n",
    "    print(f\"{phrase}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bc804",
   "metadata": {},
   "source": [
    "Time Series Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7814d7b",
   "metadata": {},
   "source": [
    "How does the publication frequency vary over time? Are there spikes in article publications related to specific market events?\n",
    "Analysis of publishing times might reveal if thereâ€™s a specific time when most news is released, which could be crucial for traders and automated trading systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf104fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Publication Frequency Over Time\n",
    "# Resample to daily frequency and count articles\n",
    "daily_freq = df.resample('D').size()\n",
    "print(\"Daily Publication Frequency:\")\n",
    "print(daily_freq.head(10))\n",
    "\n",
    "# Plot daily frequency\n",
    "helper.line_plot(daily_freq.reset_index(), x='date', y=0, title='Daily Publication Frequency Over Time')\n",
    "\n",
    "# 2. Identify Spikes (e.g., days with significantly higher publications)\n",
    "# Calculate rolling mean and std for anomaly detection\n",
    "rolling_mean = daily_freq.rolling(window=7).mean()\n",
    "rolling_std = daily_freq.rolling(window=7).std()\n",
    "threshold = rolling_mean + 2 * rolling_std\n",
    "spikes = daily_freq[daily_freq > threshold]\n",
    "print(\"\\nPotential Spike Days (above 2 std from 7-day rolling mean):\")\n",
    "print(spikes.head(10))\n",
    "\n",
    "# Note: To relate to market events, you would need external data (e.g., stock prices, earnings dates).\n",
    "# For now, this identifies statistical spikes. Manually check headlines on spike dates for events.\n",
    "\n",
    "# 3. Analysis of Publishing Times\n",
    "# Extract hour of day from index\n",
    "df['hour'] = df.index.hour\n",
    "hourly_counts = df.groupby('hour').size()\n",
    "print(\"\\nPublication Counts by Hour of Day:\")\n",
    "print(hourly_counts)\n",
    "\n",
    "# Plot hourly distribution\n",
    "helper.bar(hourly_counts.reset_index(), x='hour', y=0, title='Publications by Hour of Day')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89820a",
   "metadata": {},
   "source": [
    "Publisher Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1107d0",
   "metadata": {},
   "source": [
    "- Which publishers contribute most to the news feed? Is there a difference in the type of news they report?\n",
    "\n",
    "- If email addresses are used as publisher names, identify unique domains to see if certain organizations contribute more frequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publisher Analysis\n",
    "\n",
    "# 1. Which publishers contribute most to the news feed?\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "print(\"Top 10 Publishers by Number of Articles:\")\n",
    "print(publisher_counts.head(10))\n",
    "\n",
    "# To see if there's a difference in the type of news they report,\n",
    "# let's look at the most common words in headlines for the top 3 publishers\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_common_words(headlines, top_n=10):\n",
    "    all_words = []\n",
    "    for headline in headlines:\n",
    "        words = re.findall(r'\\b\\w+\\b', headline.lower())\n",
    "        all_words.extend(words)\n",
    "    # Remove common stop words (basic list)\n",
    "    stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'])\n",
    "    filtered_words = [word for word in all_words if word not in stop_words and len(word) > 2]\n",
    "    return Counter(filtered_words).most_common(top_n)\n",
    "\n",
    "top_publishers = publisher_counts.head(3).index\n",
    "for publisher in top_publishers:\n",
    "    headlines = df[df['publisher'] == publisher]['headline']\n",
    "    common_words = get_common_words(headlines)\n",
    "    print(f\"\\nCommon words for {publisher}:\")\n",
    "    for word, count in common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# 2. If email addresses are used as publisher names, identify unique domains\n",
    "email_publishers = df['publisher'].str.contains('@', na=False)\n",
    "if email_publishers.any():\n",
    "    domains = df[email_publishers]['publisher'].str.split('@').str[1].str.split('.').str[0]\n",
    "    unique_domains = domains.value_counts()\n",
    "    print(\"\\nUnique domains from email publishers:\")\n",
    "    print(unique_domains)\n",
    "else:\n",
    "    print(\"\\nNo email addresses found in publisher names.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
