{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News-Stock Correlation Analysis & Prediction\n",
    "\n",
    "This notebook performs a comprehensive analysis of the relationship between news sentiment and stock price movements. It includes:\n",
    "\n",
    "1. **Data Preparation**: Aligning news and stock data\n",
    "2. **Sentiment Analysis**: Using NLTK VADER to score headlines\n",
    "3. **Correlation Analysis**: Statistical tests (Pearson, Spearman) and lagged effects\n",
    "4. **Predictive Modeling**: Training ML models to predict stock movement from sentiment\n",
    "5. **Model Evaluation**: Assessing performance with advanced metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vader_lexicon...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import custom modules\n",
    "from src.fnsa.data.alignment import normalize_dates, merge_news_stock_data, prepare_ml_features, validate_date_alignment\n",
    "from src.fnsa.features.sentiment import setup_nltk_resources, batch_sentiment_analysis, aggregate_daily_sentiment\n",
    "from src.fnsa.features.stock_metrics import calculate_daily_returns, calculate_volatility, create_target_variable\n",
    "from src.fnsa.analysis.correlation import calculate_pearson_correlation, calculate_lagged_correlation\n",
    "from src.fnsa.models.sentiment_model import SentimentStockPredictor\n",
    "from src.fnsa.models.evaluator import evaluate_classification_model, cross_validate_model\n",
    "from src.fnsa.models.model_saver import save_model\n",
    "from src.fnsa.utils.plotting import PlotHelper\n",
    "from src.fnsa.data.preprocessing import convert_to_datetime, handle_missing_values, remove_duplicates\n",
    "\n",
    "\n",
    "# Initialize plotter\n",
    "plotter = PlotHelper()\n",
    "\n",
    "# Download NLTK resources\n",
    "setup_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1407328 news items\n",
      "Loaded 3774 stock records for AAPL\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"2020-05-22 00:00:00\" doesn't match format \"%Y-%m-%d %H:%M:%S%z\", at position 10. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: Data files not found. Please ensure data is in ../data/raw/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Normalize Dates\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m news_df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.dt.date\n\u001b[32m     15\u001b[39m stock_df = normalize_dates(stock_df, \u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDate ranges:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\BackUp\\web-projects\\tenx\\finance-challange-week1\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1068\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1066\u001b[39m             result = arg.tz_localize(\u001b[33m\"\u001b[39m\u001b[33mutc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     cache_array = \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array.empty:\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\BackUp\\web-projects\\tenx\\finance-challange-week1\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:249\u001b[39m, in \u001b[36m_maybe_cache\u001b[39m\u001b[34m(arg, format, cache, convert_listlike)\u001b[39m\n\u001b[32m    247\u001b[39m unique_dates = unique(arg)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) < \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     cache_dates = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\BackUp\\web-projects\\tenx\\finance-challange-week1\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    438\u001b[39m     arg,\n\u001b[32m    439\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    444\u001b[39m )\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\BackUp\\web-projects\\tenx\\finance-challange-week1\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:469\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    459\u001b[39m     arg,\n\u001b[32m    460\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    465\u001b[39m ) -> Index:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    471\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:583\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: time data \"2020-05-22 00:00:00\" doesn't match format \"%Y-%m-%d %H:%M:%S%z\", at position 10. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    news_df = pd.read_csv('../data/raw/raw_analyst_ratings.csv')\n",
    "    print(f\"Loaded {len(news_df)} news items\")\n",
    "    \n",
    "    # Load stock data (example for AAPL)\n",
    "    stock_df = pd.read_csv('../data/raw/AAPL.csv')\n",
    "    print(f\"Loaded {len(stock_df)} stock records for AAPL\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Data files not found. Please ensure data is in ../data/raw/\")\n",
    "\n",
    "# Normalize Dates\n",
    "news_df = convert_to_datetime(news_df, 'date')\n",
    "news_df = handle_missing_values(news_df, strategy='drop')\n",
    "news_df = remove_duplicates(news_df)\n",
    "\n",
    "stock_df = convert_to_datetime(stock_df, 'Date')\n",
    "stock_df = handle_missing_values(stock_df, strategy='drop')\n",
    "stock_df = remove_duplicates(stock_df)\n",
    "\n",
    "print(\"\\nDate ranges:\")\n",
    "print(f\"News: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(f\"Stock: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis\n",
    "\n",
    "We use NLTK's VADER analyzer to compute sentiment scores for each headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter news for AAPL\n",
    "aapl_news = news_df[news_df['stock'] == 'AAPL'].copy()\n",
    "print(f\"Processing {len(aapl_news)} headlines for AAPL...\")\n",
    "\n",
    "# Analyze sentiment (taking a sample to speed up demo if needed)\n",
    "# aapl_news = aapl_news.head(1000)  # Uncomment for quick testing\n",
    "\n",
    "sentiment_scores = batch_sentiment_analysis(aapl_news['headline'])\n",
    "aapl_news = pd.concat([aapl_news.reset_index(drop=True), sentiment_scores], axis=1)\n",
    "\n",
    "# Aggregate by day\n",
    "daily_sentiment = aggregate_daily_sentiment(aapl_news, date_col='date', ticker_col='stock')\n",
    "\n",
    "print(\"\\nDaily Sentiment Sample:\")\n",
    "daily_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sentiment Distribution\n",
    "plotter.histogram(aapl_news, column='compound', bins=50, title='Distribution of Headline Sentiment Scores')\n",
    "\n",
    "# Sentiment over time\n",
    "plotter.line(daily_sentiment, x='date', y='avg_sentiment', title='Daily Average Sentiment (AAPL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Merging sentiment data with stock returns to find correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Returns\n",
    "stock_df['daily_return'] = calculate_daily_returns(stock_df)\n",
    "\n",
    "# Merge Data\n",
    "merged_df = merge_news_stock_data(daily_sentiment, stock_df, 'AAPL', news_date_col='date', stock_date_col='Date')\n",
    "\n",
    "# Check alignment\n",
    "stats = validate_date_alignment(merged_df)\n",
    "print(\"Alignment Stats:\", stats)\n",
    "\n",
    "# Calculate Correlation\n",
    "corr_result = calculate_pearson_correlation(merged_df['avg_sentiment'], merged_df['daily_return'])\n",
    "print(f\"\\nCorrelation (Sentiment vs Returns): {corr_result['correlation']:.4f}\")\n",
    "print(f\"P-value: {corr_result['p_value']:.4f}\")\n",
    "print(f\"Significant: {corr_result['significant']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagged Correlation Analysis\n",
    "lagged_corr = calculate_lagged_correlation(merged_df['avg_sentiment'], merged_df['daily_return'], max_lag=5)\n",
    "\n",
    "print(\"\\nLagged Correlations:\")\n",
    "print(lagged_corr)\n",
    "\n",
    "# Visualize Lagged Correlation\n",
    "plotter.bar(lagged_corr, x='lag', y='correlation', title='Correlation by Time Lag (Days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictive Modeling\n",
    "\n",
    "Training a Machine Learning model to predict stock movement direction based on sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Features and Target\n",
    "X, y = prepare_ml_features(merged_df)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "# Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Train Model\n",
    "predictor = SentimentStockPredictor(model_type='random_forest', hyperparameters={'n_estimators': 200, 'max_depth': 10})\n",
    "train_info = predictor.train(X_train, y_train)\n",
    "\n",
    "print(\"\\nTraining Info:\", train_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = predictor.predict(X_test)\n",
    "y_proba = predictor.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_classification_model(y_test, y_pred, y_proba)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "for k, v in metrics.items():\n",
    "    if k != 'confusion_matrix':\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "plotter.plot_confusion_matrix(np.array(metrics['confusion_matrix']), labels=['Down', 'Up'])\n",
    "\n",
    "if metrics['roc_auc']:\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "    plotter.plot_roc_curve(fpr, tpr, metrics['roc_auc'])\n",
    "\n",
    "# Feature Importance\n",
    "importance_df = predictor.get_feature_importance()\n",
    "plotter.plot_feature_importance(importance_df['feature'], importance_df['importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = save_model(\n",
    "    predictor.pipeline, \n",
    "    base_path='../models/trained', \n",
    "    model_name='sentiment_rf_v1', \n",
    "    metadata={'metrics': {k: v for k, v in metrics.items() if k != 'confusion_matrix'}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
